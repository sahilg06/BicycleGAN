{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BicycleGAN_Blog&Code.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38YNt0i_jtmC"
      },
      "source": [
        "<h1><b>BicycleGAN</b><i> (Implementation in pytorch)</i></h1>\n",
        "\n",
        "> <h2>Multimodal Image-to-Image Translation</h2>\n",
        "\n",
        "\n",
        "<br>\n",
        "\n",
        "![BicycleGAN](img/bicyclegan.png)\n",
        "\n",
        "<h2>Introduction</h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "Deep learning techniques have made rapid progress in conditional image generation. However, most techniques in this space have focused on generating a single result. Our aim is to generate a distribution of output images given an input image.<br><br>\n",
        "Mapping from a high-dimensional input to a high-dimensional output distribution is challenging. A common approach to representing multimodality is learning a low-dimensional latent code, which should represent aspects of the possible outputs not contained in the input image.  At inference time,\n",
        "a deterministic generator uses the input image, along with stochastically sampled latent codes, to produce randomly sampled outputs.\n",
        "\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<br>\n",
        "<h2>Why BicycleGAN?</h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "A common problem in existing methods is mode collapse, where only a small number of real samples get represented in the output.<br>\n",
        "\n",
        "<h3>Mode Collapse</h3>\n",
        "\n",
        "<br>\n",
        "\n",
        "Real life data distribution are multimodal. For example, <b>MNIST</b> dataset has 10 major modes from 0 to 9. When mode collapses, very few modes are generated.\n",
        "You can simply understand it as lack of variety. However complete collapse doesn't occur often whereas partial collapse is common. Given figure explains it all. \n",
        "<br>\n",
        "\n",
        "![Mode Collapse](img/mode_collapse.png)\n",
        "\n",
        "Top row produces all the 10 modes of Mnist whereas bottom row produces only single mode (digit '6' ).<br><br>\n",
        "BicycleGan proposes a bijection between the output and latent space.<br>\n",
        "Not only the direct task of mapping the latent code (along with the input) to the output is performed but also jointly we learn an encoder from the output back to the latent space. This discourages two different latent codes from generating the same output (non-injective mapping) i.e. preventing <b>mode collapse</b>\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<br>\n",
        "<h2>What BicycleGAN does?</h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "Goal is to learn a multi-modal mapping between two image domains, for example, edges and photographs, or night and day images, etc.\n",
        "Consider the input domain  $A \\subset R^{H \\times W\\times 3}$ , which is to be mapped to an output domain $B \\subset R^{H \\times W\\times 3}$ . ( For example, consider A as edges and consider B as photographs made using those edges )<br><br>\n",
        "We are given a dataset of paired instances from these domains, $(A \\in A, B \\in B)$ which is representative of a joint distribution $p(A, B)$ . It is important to note that there could be multiple plausible paired instances $B$ that would correspond to an input instance $A$ , but the training dataset usually contains only one such pair. However, given a new instance A during test time, our model should be able to generate a diverse set of output $\\hat{B}$ 's, corresponding to different modes in the distribution $p(B|A)$ .<br><br>\n",
        "We would like to learn the mapping that could sample the output $\\hat{B}$  \n",
        "from true conditional distribution given $A$ , and produce results which are both diverse and realistic.<br><br>\n",
        "To do so, we learn a low-dimensional latent space $z \\in R^{z}$, which encapsulates the ambiguous aspects of the\n",
        "output mode which are not present in the input image. For example, a sketch of a shoe could map to a variety of colors and textures, which could get compressed in this latent code. We then learn a deterministic mapping $G : (A, z) \\Rightarrow B$ to the output. To enable stochastic sampling, we desire the latent code vector $z$ to be drawn from some prior distribution $p(z)$ ; we use a standard Gaussian distribution $N (0, I)$ in this work.\n",
        "<br>\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<br>\n",
        "<h2>Our model consists of 2 parts</h2>\n",
        "\n",
        "<br>\n",
        "\n",
        ">  <h2>Conditional Variational Autoencoder GAN: cVAE-GAN (1st part of model)\n",
        " $(B \\Rightarrow z \\Rightarrow \\hat{B}$) </h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "![cVAE-GAN](img/cvae.png)\n",
        "<br>\n",
        "\n",
        "\n",
        "*  The ground truth B is directly mapped with latent code(z) using an encoder E.\n",
        "*  The generator G then uses both the latent code and the input image A to synthesize the desired output $\\hat{B}$.\n",
        "*  The overall model can be easily understood as the reconstruction of B, with latent encoding z concatenated with the paired A in the middle, similar to an autoencoder.\n",
        "* The distribution Q(z|B) of latent code z (output of the encoder E) is dealt with a Gaussian assumption, $Q(\\mathrm{z}|\\mathrm{B})=E(\\mathrm{B})$.<br><br>\n",
        "\n",
        "\n",
        "<hr/>\n",
        "\n",
        "\n",
        "> <h3><b>cVAE-GAN objective, a conditional version of the VAE-GAN</b></h3>\n",
        "\n",
        "<br>\n",
        "\n",
        "$$G^{*},\\displaystyle \\ E^{*}=\\arg\\min_{G,E}\\max_{D}\\ \\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}(G,\\ D,\\ E)+\\lambda \\mathcal{L}{1^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}}(G,\\ E)+\\lambda_{\\mathrm{K}\\mathrm{L}}\\mathcal{L}_{\\mathrm{K}\\mathrm{L}}(E)$$\n",
        "\n",
        "<h3>where</h3> $$\\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}=\\mathrm{E}_{\\mathrm{A},\\mathrm{B}\\sim p(\\mathrm{A},\\mathrm{B})}[\\log(D(\\mathrm{A},\\ \\mathrm{B}))]+\\mathrm{E}_{\\mathrm{A},\\mathrm{B}\\sim p(\\mathrm{A},\\mathrm{B}),\\mathrm{z}\\sim E(\\mathrm{B})}[\\log(1-D(\\mathrm{A},\\ G(\\mathrm{A},\\ \\mathrm{z})))]$$\n",
        "\n",
        "<br>\n",
        "This is the typical loss function of GAN where Generator and Discriminator play a min-max game. Here Generator tries to fool the Discriminator whereas Discriminator tries to distinct the images generated by the Generator from the original ones.\n",
        "<br><br>\n",
        "\n",
        "$$\\mathcal{L}_{1}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}(G)= \\mathrm{E}_{\\mathrm{A},\\mathrm{B}\\sim p(\\mathrm{A},\\mathrm{B}),\\mathrm{z}\\sim E(\\mathrm{B})}||\\mathrm{B}-G(\\mathrm{A},\\ \\mathrm{z})||_{1}$$\n",
        "\n",
        "<br>\n",
        "To encourage the output of the generator to match the input as well as stabilize the training, we use an  $\\ell_{1}$ loss between the output and the ground truth image.\n",
        "<br><br><br>\n",
        "\n",
        "$$\\mathcal{L}_{\\mathrm{K}\\mathrm{L}}(E)=\\mathrm{E}_{\\mathrm{B}\\sim p(\\mathrm{B})}[\\mathcal{D}_{\\mathrm{K}\\mathrm{L}}(E(\\mathrm{B})||\\mathcal{N}(0,\\ I))]$$\n",
        "\n",
        "<br>\n",
        "The latent distribution encoded by $E(B)$ is encouraged to be close to a random Gaussian to enable sampling at inference time, when $\\mathrm{B}$ is not known.<br> <b>Here</b> $$\\displaystyle \\mathcal{D}_{\\mathrm{K}\\mathrm{L}}(p||q)=-\\int p(z)\\log\\frac{p(z)}{q(z)}dz$$\n",
        "\n",
        "\n",
        "<hr/>\n",
        "\n",
        "\n",
        "<br><br>\n",
        "Consider the deterministic version of this approach, i.e., dropping KLdivergence and encoding z = E(B). It is called cAE-GAN .\n",
        "There is no guarantee in cAE-GAN on the distribution of the latent space z, which makes the test-time\n",
        "sampling of z difficult.\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<h2>Conditional Latent Regressor GAN: cLR-GAN  (2nd part of the model)</h2> \n",
        "\n",
        "$({z} \\Rightarrow \\hat{B} \\Rightarrow \\hat{z})$ \n",
        "\n",
        "<br>\n",
        "\n",
        "![cLR-GAN](img/clr.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "A randomly drawn latent code z is recovered with $\\hat{\\mathrm{z}}=E(G(\\mathrm{A},\\ \\mathrm{z}))$\n",
        "<br><br>\n",
        "Encoder E here is producing a point estimate for $\\hat{\\mathrm{z}}$, whereas the encoder in the previous section was predicting a Gaussian distribution.\n",
        "<br><br>\n",
        "<h3><b>cLR-GAN objective function</b></h3>\n",
        "<br>\n",
        "$G^{*},\\displaystyle \\ E^{*}=\\arg\\min_{G,E}\\max_{D}\\ \\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}(G,\\ D)+\\lambda_{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}\\mathcal{L}_{1}^{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}(G,\\ E)$\n",
        "\n",
        "<br>\n",
        "<h3>where</h3>\n",
        "$$\\mathcal{L}_{1}^{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}(G,\\ E)=\\mathrm{E}_{\\mathrm{A}\\sim p(\\mathrm{A}),\\mathrm{z}\\sim p(\\mathrm{z})}||\\mathrm{z}-E(G(\\mathrm{A},\\ \\mathrm{z}))||_{1}$$\n",
        "<br>\n",
        "$\\hat{\\mathrm{z}}=E(G(\\mathrm{A},\\ \\mathrm{z}))$ is encouraged to be close to the randomly drawn $\\mathrm{z}$ to enable bijective mapping.\n",
        "<br><br>\n",
        "\n",
        "The discriminator loss $L_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}(G,\\ D)$  on $\\hat{\\mathrm{B}}$ is used to encourage the network to generate realistic results.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<br><br>\n",
        "<h2>Hybrid Model: BicycleGAN</h2>\n",
        "<br><br>\n",
        "Combine the cVAE-GAN and cLR-GAN objectives in the hybrid model.<br><br>\n",
        "Training is done in both directions, aiming to take advantage of both cycles<br> \n",
        "($\\mathrm{B}\\rightarrow \\mathrm{z}\\rightarrow\\hat{\\mathrm{B}}$ and $\\mathrm{z}\\rightarrow\\hat{\\mathrm{B}}\\rightarrow\\hat{\\mathrm{z}}$), hence the name BicycleGAN.<br><br>\n",
        "\n",
        "<h3>Combined Objective</h3>\n",
        "\n",
        "<br>\n",
        "\n",
        "$$\n",
        "G^{*},\\ E^{*}=\\arg\\min_{G,E}\\max\\ \\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}^{\\mathrm{V}\\mathrm{A}\\mathrm{E}}(G,\\ D,\\ E)+\\lambda \\mathcal{L}_{1^{\\mathrm{A}\\mathrm{E}}}(G,\\ E)\n",
        "+\\mathcal{L}_{\\mathrm{G}\\mathrm{A}\\mathrm{N}}(G,\\ D)+\\lambda_{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}\\mathcal{L}_{1}^{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}(G,\\ E)+\\lambda_{\\mathrm{K}\\mathrm{L}}\\mathcal{L}_{\\mathrm{K}\\mathrm{L}}(E)\\ ,\n",
        "$$\n",
        "\n",
        "<br>\n",
        "where the hyper-parameters $\\lambda, \\lambda_{\\mathrm{l}\\mathrm{a}\\mathrm{t}\\mathrm{e}\\mathrm{n}\\mathrm{t}}$, and $\\lambda_{\\mathrm{K}\\mathrm{L}}$ control the relative importance of each term.\n",
        "\n",
        "<hr/>\n",
        "\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h2>Implementation details<h2> \n",
        "\n",
        "<br>\n",
        "\n",
        "<h3>Network architecture</h3>\n",
        "<br>\n",
        "For generator G, U-Net is used, which contains an encoder-decoder\n",
        "architecture, with symmetric skip connections. The architecture has been shown to produce strong results in the unimodal image prediction setting when there is a spatial correspondence between input and output pairs.<br><br>\n",
        "\n",
        "<img src=\"img/unet.png\" alt=\"UNET\">\n",
        "\n",
        "<br>\n",
        "\n",
        "For discriminator D, generally two PatchGAN discriminators at different\n",
        "scales are used, which aim to predict real vs. fake overlapping image patches.\n",
        "<br><br><br>\n",
        "For the encoder E, these two networks are preferred: <br>\n",
        "(1) $E_{CNN}$: CNN with a few convolutional and downsampling layers .<br>\n",
        "(2) $E_{Resnet}$: a classifier with several residual block .\n",
        "\n",
        "<hr/>\n",
        "\n",
        "<br><br>\n",
        "\n",
        "<h2>Implementation in Pytorch<h2>\n",
        "\n",
        "<br>\n",
        "\n",
        "<h3>Dataset : Edges2Shoes</h3>\n",
        "\n",
        "<br>\n",
        "\n",
        "![Edges2Shoes](img/edgesToShoes.png)\n",
        "\n",
        "\n",
        "<br><br>\n",
        "An example of training data:\n",
        "<br>\n",
        "\n",
        "![training-data](img/train.jpg)\n",
        "\n",
        "<br><br>\n",
        "An example of validation data:\n",
        "<br>\n",
        "\n",
        "![validation-data](img/val.jpg)\n",
        "\n",
        "<br>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbvfl0TdL80z"
      },
      "source": [
        "<h4>Let us first import the required libraries and modules</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbzqqCBmnkVs"
      },
      "source": [
        "import os\n",
        "import sys\n",
        "import glob\n",
        "import math\n",
        "import time\n",
        "import random\n",
        "import itertools\n",
        "import datetime\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets\n",
        "from torch.autograd import Variable\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.models import resnet18\n",
        "from torchvision.utils import save_image\n",
        "import torchvision.transforms as transforms"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OjSSd6DiTkrI"
      },
      "source": [
        "<h4>Custom Dataloader</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1_VmVI6RRrq4",
        "cellView": "code"
      },
      "source": [
        "class ImageDataset(Dataset):\n",
        "    def __init__(self, root, input_shape, mode=\"train\"):\n",
        "        self.transform = transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(input_shape[-2:], Image.BICUBIC),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "            ]\n",
        "        )\n",
        "\n",
        "        self.files = sorted(glob.glob(os.path.join(root, mode) + \"/*.*\"))\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "\n",
        "        img = Image.open(self.files[index % len(self.files)])\n",
        "        w, h = img.size\n",
        "        img_A = img.crop((0, 0, w / 2, h))\n",
        "        img_B = img.crop((w / 2, 0, w, h))\n",
        "\n",
        "        if np.random.random() < 0.5:\n",
        "            img_A = Image.fromarray(np.array(img_A)[:, ::-1, :], \"RGB\")\n",
        "            img_B = Image.fromarray(np.array(img_B)[:, ::-1, :], \"RGB\")\n",
        "\n",
        "        img_A = self.transform(img_A)\n",
        "        img_B = self.transform(img_B)\n",
        "\n",
        "        return {\"A\": img_A, \"B\": img_B}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.files)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCwn-IeX9YCP"
      },
      "source": [
        "<h4>Function to initialize weights for convolution and batchnorm layers</h4>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMduxeHb7LeT"
      },
      "source": [
        "def weights_init_normal(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find(\"Conv\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 0.0, 0.02)\n",
        "    elif classname.find(\"BatchNorm2d\") != -1:\n",
        "        torch.nn.init.normal_(m.weight.data, 1.0, 0.02)\n",
        "        torch.nn.init.constant_(m.bias.data, 0.0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A-JUwSFnU0lQ"
      },
      "source": [
        "<h3>UNetDown Block</h3>\n",
        "<br>(Block for downsampling layers of Unet)<br>\n",
        "<br>Small unit block consists of  (convolution layer - normalization layer - non linearity layer)<br>\n",
        "\n",
        "\n",
        "    Parameters\n",
        "\n",
        "    1. in_size : Input dimension(channels number) \n",
        "    2. out_size : Output dimension(channels number)\n",
        "    3. normalize : If it is true add Batch Normalization layer, otherwise skip this layer\n",
        "    4. dropout : probability for dropping a unit\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RjkwRL_T4YN"
      },
      "source": [
        "class UNetDown(nn.Module):\n",
        "    def __init__(self, in_size, out_size, normalize=True, dropout=0.0):\n",
        "        super(UNetDown, self).__init__()\n",
        "        layers = [nn.Conv2d(in_size, out_size, 3, stride=2, padding=1, bias=False)]\n",
        "        if normalize:\n",
        "            layers.append(nn.BatchNorm2d(out_size, 0.8))\n",
        "        layers.append(nn.LeakyReLU(0.2))\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XZ_KnX_XVrZ"
      },
      "source": [
        "<h3>UNetUp Block</h3>\n",
        "<br>(Block for Upsampling layers of Unet)<br>\n",
        "<br>Small unit block consists of (upsampling layer - convolution layer- normalization layer - non linearity layer)<br>\n",
        "    \n",
        "    Parameters\n",
        "\n",
        "    1. in_dim : Input dimension(channels number)\n",
        "    2. out_dim : Output dimension(channels number)\n",
        "    3. skip_input (in forward method) : skip connection from corresponding downsampling layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwJSqz1nW5QI"
      },
      "source": [
        "class UNetUp(nn.Module):\n",
        "    def __init__(self, in_size, out_size):\n",
        "        super(UNetUp, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2),\n",
        "            nn.Conv2d(in_size, out_size, 3, stride=1, padding=1, bias=False),\n",
        "            nn.BatchNorm2d(out_size, 0.8),\n",
        "            nn.ReLU(inplace=True),\n",
        "        )\n",
        "\n",
        "    def forward(self, x, skip_input):\n",
        "        x = self.model(x)\n",
        "        x = torch.cat((x, skip_input), 1)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5mXlQaimYJIH"
      },
      "source": [
        "<h2>Generator</h2>\n",
        "\n",
        "> U-Net Generator \n",
        "\n",
        "<br>\n",
        "Downsampled activation volume and upsampled activation volume which have same width and height make pairs and they are concatenated when upsampling.<br>\n",
        "\n",
        "    Pairs : (up_1, down_6)\n",
        "            (up_2, down_5)  \n",
        "            (up_3, down_4) \n",
        "            (up_4, down_3) \n",
        "            (up_5, down_2) \n",
        "            (up_6, down_1)\n",
        "            down_7 doesn't have a partener.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h5lLjo3PYFWR"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, latent_dim, img_shape):\n",
        "        super(Generator, self).__init__()\n",
        "        channels, self.h, self.w = img_shape\n",
        "\n",
        "        self.fc = nn.Linear(latent_dim, self.h * self.w)\n",
        "\n",
        "        self.down1 = UNetDown(channels + 1, 64, normalize=False)\n",
        "        self.down2 = UNetDown(64, 128)\n",
        "        self.down3 = UNetDown(128, 256)\n",
        "        self.down4 = UNetDown(256, 512)\n",
        "        self.down5 = UNetDown(512, 512)\n",
        "        self.down6 = UNetDown(512, 512)\n",
        "        self.down7 = UNetDown(512, 512, normalize=False)\n",
        "        self.up1 = UNetUp(512, 512)\n",
        "        self.up2 = UNetUp(1024, 512)\n",
        "        self.up3 = UNetUp(1024, 512)\n",
        "        self.up4 = UNetUp(1024, 256)\n",
        "        self.up5 = UNetUp(512, 128)\n",
        "        self.up6 = UNetUp(256, 64)\n",
        "\n",
        "        self.final = nn.Sequential(\n",
        "            nn.Upsample(scale_factor=2), nn.Conv2d(128, channels, 3, stride=1, padding=1), nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, x, z):\n",
        "        # Propogate noise through fc layer and reshape to img shape\n",
        "        #x:(N,3,128,128) z:(N,8)\n",
        "        z = self.fc(z).view(z.size(0), 1, self.h, self.w)#z:(N,1,128,128)\n",
        "        \n",
        "        #concating (x and z): (N,4,128,128)\n",
        "        d1 = self.down1(torch.cat((x, z), 1)) #d1:(N,64,64,64)\n",
        "        d2 = self.down2(d1)         #d2:(N,128,32,32)\n",
        "        d3 = self.down3(d2)         #d3:(N,256,16,16)\n",
        "        d4 = self.down4(d3)         #d4:(N,512,8,8)\n",
        "        d5 = self.down5(d4)         #d5:(N,512,4,4)\n",
        "        d6 = self.down6(d5)         #d6:(N,512,2,2)\n",
        "        d7 = self.down7(d6)         #d7:(N,512,1,1)\n",
        "        u1 = self.up1(d7, d6)       #u1:(N,1024,2,2)\n",
        "        u2 = self.up2(u1, d5)       #u2:(N,1024,4,4)\n",
        "        u3 = self.up3(u2, d4)       #u3:(N,1024,8,8)\n",
        "        u4 = self.up4(u3, d3)       #u4:(N,512,16,16)\n",
        "        u5 = self.up5(u4, d2)       #u5:(N,256,32,32)\n",
        "        u6 = self.up6(u5, d1)       #u6:(N,128,64,64)\n",
        "\n",
        "        return self.final(u6)       #final:(N,3,128,128)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RAdruM9yab27"
      },
      "source": [
        "<h2>MultiDiscriminator </h2>\n",
        "<br>\n",
        "  It uses multiple discriminators, which return different output sizes (i.e. different local probabilities)<br>\n",
        "\n",
        "\n",
        "    disc_1 : (N, channels, 128, 128) -> (N, 1, 8, 8)\n",
        "    disc_2 : (N, channels, 64, 64) -> (N, 1, 4, 4)\n",
        "    disc_3 : (N, channels, 32, 32) -> (N, 1, 2, 2)\n",
        "\n",
        "\n",
        "In training, the generator needs to fool all the discriminators and it makes the generator more robust."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RTY0_9yUaUc-"
      },
      "source": [
        "class MultiDiscriminator(nn.Module):\n",
        "    def __init__(self, input_shape):\n",
        "        super(MultiDiscriminator, self).__init__()\n",
        "\n",
        "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
        "            \"\"\"Returns downsampling layers of each discriminator block\"\"\"\n",
        "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm2d(out_filters, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2))\n",
        "            return layers\n",
        "\n",
        "        channels, _, _ = input_shape\n",
        "        # Extracts discriminator models\n",
        "        self.models = nn.ModuleList()\n",
        "        for i in range(3):\n",
        "            self.models.add_module(\n",
        "                \"disc_%d\" % i,\n",
        "                nn.Sequential(\n",
        "                    *discriminator_block(channels, 64, normalize=False),\n",
        "                    *discriminator_block(64, 128),\n",
        "                    *discriminator_block(128, 256),\n",
        "                    *discriminator_block(256, 512),\n",
        "                    nn.Conv2d(512, 1, 3, padding=1)\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        self.downsample = nn.AvgPool2d(3, stride=2, padding=[1, 1], count_include_pad=False)\n",
        "\n",
        "    def compute_loss(self, x, gt):\n",
        "        \"\"\"Computes the MSE between model output and scalar gt\"\"\"\n",
        "        loss = sum([torch.mean((out - gt) ** 2) for out in self.forward(x)])\n",
        "        return loss\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for m in self.models:\n",
        "            outputs.append(m(x))\n",
        "            x = self.downsample(x)\n",
        "        return outputs\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kKtOPieCU9DE"
      },
      "source": [
        "<h2>Encoder</h2>\n",
        "<br>\n",
        "Output is mu and log(var) for reparameterization trick used in Variation Auto Encoder.<br>Encoding is done in this order.\n",
        "\n",
        "\n",
        "    1. Use this encoder and get mu and log_var\n",
        "    2. std = exp(log(var / 2))\n",
        "    3. random_z = N(0, 1)\n",
        "    4. encoded_z = random_z * std + mu (Reparameterization trick)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KPjnXSk0D-BD"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, latent_dim, input_shape):\n",
        "        super(Encoder, self).__init__()\n",
        "        resnet18_model = resnet18(pretrained=False)\n",
        "        self.feature_extractor = nn.Sequential(*list(resnet18_model.children())[:-3])\n",
        "        self.pooling = nn.AvgPool2d(kernel_size=8, stride=8, padding=0)\n",
        "        # Output is mu and log(var) for reparameterization trick used in VAEs\n",
        "        self.fc_mu = nn.Linear(256, latent_dim)\n",
        "        self.fc_logvar = nn.Linear(256, latent_dim)\n",
        "\n",
        "    def forward(self, img):\n",
        "        #img : (N, 3, 128, 128)\n",
        "        out = self.feature_extractor(img)  # out : (N, 256, 8, 8)\n",
        "        out = self.pooling(out)            # out : (N, 256, 1, 1)\n",
        "        out = out.view(out.size(0), -1)    # out : (N, 256)\n",
        "        mu = self.fc_mu(out)               # mu : (N, latent_dim)\n",
        "        logvar = self.fc_logvar(out)       # logvar : (N, latent_dim)\n",
        "        return mu, logvar\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmtO9gzpFf-a"
      },
      "source": [
        "<h2>Reparameterization trick</h2>\n",
        "<br>If we don't perform reparameterization and simply take a sample from the distribution  $\\mathcal{N}(\\mu,\\,\\sigma^{2})\\,$ , then we cannot backprop the error through the layer that samples z from the distribution as it is a NON-Continuous operation. Hence reparameterization comes to role"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRGy0wVWFg2b"
      },
      "source": [
        "def reparameterization(mu, logvar):\n",
        "    std = torch.exp(logvar / 2)\n",
        "    sampled_z = Variable(Tensor(np.random.normal(0, 1, (mu.size(0), latent_dim))))\n",
        "    z = sampled_z * std + mu\n",
        "    return z\n",
        "    # z : (N, latent_dim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o10yGz4V_jRq"
      },
      "source": [
        "<h3> Assigning default values to arguments and initializing .</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6eBIsoGN_kAv"
      },
      "source": [
        "epoch = 0                      #epoch to start training from\n",
        "n_epochs = 10                 #number of epochs of training\n",
        "dataset_name = \"edges2shoes\"   #name of the dataset\n",
        "batch_size = 8                 #size of the batches\n",
        "lr = 0.0002                    #adam: learning rate\n",
        "b1 = 0.5                       #adam: decay of first order momentum of gradient\n",
        "b2 = 0.999                     #adam: decay of second order momentum of gradient\n",
        "n_cpu = 8                      #number of cpu threads to use during batch generation\n",
        "img_height = 128               #size of image height\n",
        "img_width = 128                #size of image width\n",
        "channels = 3                   #number of image channels\n",
        "latent_dim = 8                 #number of latent codes\n",
        "sample_interval = 400          #interval between saving generator samples\n",
        "checkpoint_interval = -1       #interval between model checkpoints\n",
        "lambda_pixel = 10              #pixelwise loss weight\n",
        "lambda_latent = 0.5            #latent loss weight\n",
        "lambda_kl = 0.01               #kullback-leibler loss weight\n",
        "mae_loss = torch.nn.L1Loss()   #Mean Absolute error loss\n",
        "\n",
        "\n",
        "input_shape = (channels, img_height, img_width)       #shape of input image (tuple)\n",
        "\n",
        "cuda = True if torch.cuda.is_available() else False   #availability of GPU\n",
        "Tensor = torch.cuda.FloatTensor if cuda else torch.Tensor\n",
        "\n",
        "\n",
        "generator = Generator(latent_dim, input_shape)    #Initialize generator\n",
        "encoder = Encoder(latent_dim, input_shape)        #Initialize encoder\n",
        "D_VAE = MultiDiscriminator(input_shape)           #initialize discriminators\n",
        "D_LR = MultiDiscriminator(input_shape)\n",
        "\n",
        "\n",
        "if cuda:\n",
        "    generator = generator.cuda()\n",
        "    encoder.cuda()\n",
        "    D_VAE = D_VAE.cuda()\n",
        "    D_LR = D_LR.cuda()\n",
        "    mae_loss.cuda()\n",
        "\n",
        "    # Initialize weights\n",
        "    generator.apply(weights_init_normal)\n",
        "    D_VAE.apply(weights_init_normal)\n",
        "    D_LR.apply(weights_init_normal)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YVNPh4WFJcMl"
      },
      "source": [
        "<h3>Making the directory where output images will get saved"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTHSwefeUN1W"
      },
      "source": [
        "os.makedirs(\"images/%s\" % dataset_name, exist_ok=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DeK6SOsLYEk1"
      },
      "source": [
        "<h3>Optimizers : </h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlLG0qm4YDjz"
      },
      "source": [
        "optimizer_E = torch.optim.Adam(encoder.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D_VAE = torch.optim.Adam(D_VAE.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_D_LR = torch.optim.Adam(D_LR.parameters(), lr=lr, betas=(b1, b2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8r6VivrSYSxG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PUhbA7W_UgRh"
      },
      "source": [
        "dataloader = DataLoader(\n",
        "    ImageDataset(\"../../data/%s\" % dataset_name, input_shape),\n",
        "    batch_size=batch_size,\n",
        "    shuffle=True,\n",
        "    num_workers=n_cpu,\n",
        ")\n",
        "val_dataloader = DataLoader(\n",
        "    ImageDataset(\"../../data/%s\" % dataset_name, input_shape, mode=\"val\"),\n",
        "    batch_size=8,\n",
        "    shuffle=True,\n",
        "    num_workers=1,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TSGp6ba8d9iF"
      },
      "source": [
        "<h3>Saves a generated sample from the validation set</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BFpFpQ1ld8Rp"
      },
      "source": [
        "def sample_images(batches_done):\n",
        "\n",
        "    generator.eval()\n",
        "    imgs = next(iter(val_dataloader))\n",
        "    img_samples = None\n",
        "    for img_A, img_B in zip(imgs[\"A\"], imgs[\"B\"]):\n",
        "\n",
        "        # Repeat input image by number of desired columns\n",
        "        real_A = img_A.view(1, *img_A.shape).repeat(latent_dim, 1, 1, 1)\n",
        "        real_A = Variable(real_A.type(Tensor))\n",
        "\n",
        "        # Sample latent representations\n",
        "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (latent_dim, latent_dim))))\n",
        "        # Generate samples\n",
        "        fake_B = generator(real_A, sampled_z)\n",
        "        # Concatenate samples horisontally\n",
        "        fake_B = torch.cat([x for x in fake_B.data.cpu()], -1)\n",
        "        img_sample = torch.cat((img_A, fake_B), -1)\n",
        "        img_sample = img_sample.view(1, *img_sample.shape)\n",
        "        # Concatenate with previous samples vertically\n",
        "        img_samples = img_sample if img_samples is None else torch.cat((img_samples, img_sample), -2)\n",
        "    save_image(img_samples, \"images/%s/%s.png\" % (dataset_name, batches_done), nrow=8, normalize=True)\n",
        "    generator.train()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wb5qh55bfWX6"
      },
      "source": [
        "<h3>TRAINING</h3>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhMXOWP7fXU3",
        "outputId": "2586f90c-7e64-4386-f028-bf79bb5efffc"
      },
      "source": [
        "# Adversarial loss\n",
        "valid = 1\n",
        "fake = 0\n",
        "\n",
        "prev_time = time.time()\n",
        "for epoch in range(epoch, n_epochs):\n",
        "    for i, batch in enumerate(dataloader):\n",
        "\n",
        "        # Set model input\n",
        "        real_A = Variable(batch[\"A\"].type(Tensor))\n",
        "        real_B = Variable(batch[\"B\"].type(Tensor))\n",
        "\n",
        "        # -------------------------------\n",
        "        #  Train Generator and Encoder\n",
        "        # -------------------------------\n",
        "\n",
        "        optimizer_E.zero_grad()\n",
        "        optimizer_G.zero_grad()\n",
        "\n",
        "        # ----------\n",
        "        # cVAE-GAN\n",
        "        # ----------\n",
        "\n",
        "        # Produce output using encoding of B (cVAE-GAN)\n",
        "        mu, logvar = encoder(real_B)\n",
        "        encoded_z = reparameterization(mu, logvar)\n",
        "        fake_B = generator(real_A, encoded_z)\n",
        "\n",
        "        # Pixelwise loss of translated image by VAE\n",
        "        loss_pixel = mae_loss(fake_B, real_B)\n",
        "        # Kullback-Leibler divergence of encoded B\n",
        "        loss_kl = 0.5 * torch.sum(torch.exp(logvar) + mu ** 2 - logvar - 1)\n",
        "        # Adversarial loss\n",
        "        loss_VAE_GAN = D_VAE.compute_loss(fake_B, valid)\n",
        "\n",
        "        # ---------\n",
        "        # cLR-GAN\n",
        "        # ---------\n",
        "\n",
        "        # Produce output using sampled z (cLR-GAN)\n",
        "        sampled_z = Variable(Tensor(np.random.normal(0, 1, (real_A.size(0), latent_dim))))\n",
        "        _fake_B = generator(real_A, sampled_z)\n",
        "        # cLR Loss: Adversarial loss\n",
        "        loss_LR_GAN = D_LR.compute_loss(_fake_B, valid)\n",
        "\n",
        "        # ----------------------------------\n",
        "        # Total Loss (Generator + Encoder)\n",
        "        # ----------------------------------\n",
        "\n",
        "        loss_GE = loss_VAE_GAN + loss_LR_GAN + lambda_pixel * loss_pixel + lambda_kl * loss_kl\n",
        "\n",
        "        loss_GE.backward(retain_graph=True)\n",
        "        optimizer_E.step()\n",
        "\n",
        "        # ---------------------\n",
        "        # Generator Only Loss\n",
        "        # ---------------------\n",
        "\n",
        "        # Latent L1 loss\n",
        "        _mu, _ = encoder(_fake_B)\n",
        "        loss_latent = lambda_latent * mae_loss(_mu, sampled_z)\n",
        "\n",
        "        loss_latent.backward()\n",
        "        optimizer_G.step()\n",
        "\n",
        "        # ----------------------------------\n",
        "        #  Train Discriminator (cVAE-GAN)\n",
        "        # ----------------------------------\n",
        "\n",
        "        optimizer_D_VAE.zero_grad()\n",
        "\n",
        "        loss_D_VAE = D_VAE.compute_loss(real_B, valid) + D_VAE.compute_loss(fake_B.detach(), fake)\n",
        "\n",
        "        loss_D_VAE.backward()\n",
        "        optimizer_D_VAE.step()\n",
        "\n",
        "        # ---------------------------------\n",
        "        #  Train Discriminator (cLR-GAN)\n",
        "        # ---------------------------------\n",
        "\n",
        "        optimizer_D_LR.zero_grad()\n",
        "\n",
        "        loss_D_LR = D_LR.compute_loss(real_B, valid) + D_LR.compute_loss(_fake_B.detach(), fake)\n",
        "\n",
        "        loss_D_LR.backward()\n",
        "        optimizer_D_LR.step()\n",
        "\n",
        "        # --------------\n",
        "        #  Log Progress\n",
        "        # --------------\n",
        "\n",
        "        # Determine approximate time left\n",
        "        batches_done = epoch * len(dataloader) + i\n",
        "        batches_left = n_epochs * len(dataloader) - batches_done\n",
        "        time_left = datetime.timedelta(seconds=batches_left * (time.time() - prev_time))\n",
        "        prev_time = time.time()\n",
        "\n",
        "        # Print log\n",
        "        sys.stdout.write(\n",
        "            \"\\r[Epoch %d/%d] [Batch %d/%d] [D VAE_loss: %f, LR_loss: %f] [G loss: %f, pixel: %f, kl: %f, latent: %f] ETA: %s\"\n",
        "            % (\n",
        "                epoch,\n",
        "                n_epochs,\n",
        "                i,\n",
        "                len(dataloader),\n",
        "                loss_D_VAE.item(),\n",
        "                loss_D_LR.item(),\n",
        "                loss_GE.item(),\n",
        "                loss_pixel.item(),\n",
        "                loss_kl.item(),\n",
        "                loss_latent.item(),\n",
        "                time_left,\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if batches_done % sample_interval == 0:\n",
        "            sample_images(batches_done)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 4/10] [Batch 2860/6229] [D VAE_loss: 0.944683, LR_loss: 1.485869] [G loss: 4.076747, pixel: 0.139999, kl: 10.436765, latent: 0.338610] ETA: 11:49:17.409138"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNquHopCcxsH"
      },
      "source": [
        "<h2>Results (Output Images):</h2>\r\n",
        "<br><br>\r\n",
        "<h4>After 0 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_0](img/0.png)\r\n",
        "\r\n",
        "<br>\r\n",
        "<h4>After 1200 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_1200](img/1200.png)\r\n",
        "\r\n",
        "<br>\r\n",
        "<h4>After 3200 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_3200](img/3200.png)\r\n",
        "\r\n",
        "<br>\r\n",
        "<h4>After 8800 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_8800](img/8800.png)\r\n",
        "\r\n",
        "<br>\r\n",
        "<h4>After 11600 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_11600](img/11600.png)\r\n",
        "\r\n",
        "<br>\r\n",
        "<h4>After 18800 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_18800](img/18800.png)\r\n",
        "\r\n",
        "<br>\r\n",
        "<h4>After 20400 batches </h4>\r\n",
        "<br>\r\n",
        "\r\n",
        "![Result_20400](img/20400.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz1CeIZreEli"
      },
      "source": [
        "<h3>References</h3>\n",
        "<br><br>\n",
        "\n",
        "@misc { zhu2018multimodal ,<br>\n",
        "&emsp;&emsp;&emsp;&emsp;title = { Toward Multimodal Image-to-Image Translation }, <br>\n",
        "&emsp;&emsp;&emsp;&emsp;author = { Jun-Yan Zhu and Richard Zhang and Deepak Pathak and Trevor Darrell and Alexei A. Efros and Oliver Wang and Eli Shechtman },<br>\n",
        "&emsp;&emsp;&emsp;&emsp;year = { 2018 },<br>\n",
        "&emsp;&emsp;&emsp;&emsp;eprint = { 1711.11586 },<br>\n",
        "&emsp;&emsp;&emsp;&emsp;archivePrefix = { arXiv },<br>\n",
        "&emsp;&emsp;&emsp;&emsp;primaryClass = { cs.CV }<br>\n",
        "}\n",
        "<br><br>\n",
        "* https://github.com/eriklindernoren/PyTorch-GAN"
      ]
    }
  ]
}